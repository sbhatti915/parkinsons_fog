{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914e7306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:33.520287Z",
     "iopub.status.busy": "2023-04-05T17:02:33.519490Z",
     "iopub.status.idle": "2023-04-05T17:02:42.213112Z",
     "shell.execute_reply": "2023-04-05T17:02:42.211963Z"
    },
    "papermill": {
     "duration": 8.70483,
     "end_time": "2023-04-05T17:02:42.215818",
     "exception": false,
     "start_time": "2023-04-05T17:02:33.510988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "from scipy import signal\n",
    "import glob\n",
    "\n",
    "CATEGORIES = ['StartHesitation','Turn','Walking'] # 3 classes\n",
    "FEATNAMES = ['AccV','AccML','AccAP']\n",
    "mlb = MultiLabelBinarizer()\n",
    "lenc = LabelEncoder()\n",
    "mlb.fit([CATEGORIES])\n",
    "lenc.fit(CATEGORIES)\n",
    "\n",
    "SOURCE_DIR = '/kaggle/input/'\n",
    "SAVE_DIR = '/kaggle/working/'\n",
    "DATASOURCE = 'realworld' # options: lab | realworld | lab_and_realworld\n",
    "TASK = 'binary' # options: binary | multiclass\n",
    "SAMPLES = 150 # lab = 192, realworld = 150\n",
    "jump = 30 # lab = 38 (80% overlap), realworld = 30\n",
    "\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec941a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:42.227153Z",
     "iopub.status.busy": "2023-04-05T17:02:42.226557Z",
     "iopub.status.idle": "2023-04-05T17:02:42.254787Z",
     "shell.execute_reply": "2023-04-05T17:02:42.253874Z"
    },
    "papermill": {
     "duration": 0.036344,
     "end_time": "2023-04-05T17:02:42.257011",
     "exception": false,
     "start_time": "2023-04-05T17:02:42.220667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# populate dataframe only with data associated with labels (0, 1, 2) for the 3 classes\n",
    "def get_multiclass_df(FILENAMES,metadata_df,DATASOURCE):\n",
    "    df = pd.DataFrame()\n",
    "    for filename in tqdm(FILENAMES):\n",
    "        data = pd.read_csv(filename)\n",
    "        if DATASOURCE == 'realworld': # only consider rows with unambiguous annotations\n",
    "            data = data[(data['Valid']==True) & (data['Task']==True)]\n",
    "            data.reset_index(inplace=True,drop=True)\n",
    "        nevents = data[CATEGORIES].sum().sum()\n",
    "        if nevents == 0: # skip if subject has no event whatsoever\n",
    "            continue\n",
    "        labels = mlb.inverse_transform(np.array(data[CATEGORIES])) # returns class tuple\n",
    "        eventId = filename.split('/')[-1].split('.csv')[0]\n",
    "        subjectId = metadata_df[metadata_df['Id']==eventId]['Subject'].item()\n",
    "        subjectId = eventId\n",
    "        \n",
    "        for category in CATEGORIES:\n",
    "            condition = data[category]==1\n",
    "            category_df = data[FEATNAMES][condition]\n",
    "            category_df['label'] = lenc.transform(pd.DataFrame(labels)[condition].iloc[:,0])\n",
    "            category_df['subject'] = subjectId\n",
    "            category_df['series'] = eventId\n",
    "            category_df.reset_index(inplace=True,drop=True)\n",
    "            df = pd.concat((df,category_df),0)\n",
    "\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "#%%\n",
    "# obtain samples for the background class\n",
    "#events_df = pd.read_csv('/home/danikiyasseh/datasets/tlvmc-parkinsons-freezing-gait-prediction/events.csv')\n",
    "def get_background_df(FILENAMES,metadata_df,DATASOURCE):\n",
    "    background_df = pd.DataFrame()\n",
    "    for filename in tqdm(FILENAMES):\n",
    "        data = pd.read_csv(filename)\n",
    "        if DATASOURCE == 'realworld': # only consider rows with unambiguous annotations\n",
    "            data = data[(data['Valid']==True) & (data['Task']==True)]\n",
    "            data.reset_index(inplace=True,drop=True)\n",
    "        labels = [-1]*data.shape[0]\n",
    "        eventId = filename.split('/')[-1].split('.csv')[0]\n",
    "        subjectId = metadata_df[metadata_df['Id']==eventId]['Subject'].item()\n",
    "        subjectId = eventId\n",
    "    \n",
    "        condition = ~data[CATEGORIES].any(axis=1)\n",
    "        category_df = data[FEATNAMES][condition]\n",
    "        category_df['label'] = pd.DataFrame(labels)[condition]\n",
    "        category_df['subject'] = subjectId\n",
    "        category_df['series'] = eventId\n",
    "        category_df.reset_index(inplace=True,drop=True)\n",
    "        background_df = pd.concat((background_df,category_df),0)\n",
    "    \n",
    "    background_df['label'] = background_df['label'].astype(int)\n",
    "    return background_df\n",
    "\n",
    "#%%\n",
    "# prepare samples in dictionary format\n",
    "def get_data_dict(df,unitConversion=1):\n",
    "    data_dict = dict()\n",
    "    for subject in tqdm(df['subject'].unique()):\n",
    "        data_dict[subject] = defaultdict(list)\n",
    "        subject_df = df[df['subject']==subject]\n",
    "        for series in subject_df['series'].unique():\n",
    "            series_df = subject_df[subject_df['series']==series]\n",
    "            for category in series_df['label'].unique():\n",
    "                category_df = series_df[series_df['label']==category]\n",
    "                if category_df.shape[0] >= SAMPLES: # at least this many samples for this subject from this category\n",
    "            \n",
    "                    start = 0\n",
    "                    end = start + SAMPLES\n",
    "                    while end <= category_df.shape[0]:\n",
    "                        chunk_category_df = category_df[start:end]\n",
    "                        chunk_category_arr = np.array(chunk_category_df[FEATNAMES]) * unitConversion # SAMPLES x NFEATS\n",
    "                        data_dict[subject][category].append(chunk_category_arr)\n",
    "                        start = start + jump\n",
    "                        end = start + SAMPLES\n",
    "                \n",
    "        for category in subject_df['label'].unique():\n",
    "            if len(data_dict[subject][category]) == 0:\n",
    "                data_dict[subject].pop(category)\n",
    "            else:\n",
    "                data_dict[subject][category] = np.stack(data_dict[subject][category]) # NCHUNKS x SAMPLES x NFEATS\n",
    "    \n",
    "    subjects_to_keep = [] # remove empty entries\n",
    "    for subject,data in data_dict.items():\n",
    "        if data != dict():\n",
    "            subjects_to_keep.extend([subject])\n",
    "    new_data_dict = {subject:data_dict[subject] for subject in subjects_to_keep}\n",
    "    return new_data_dict\n",
    "\n",
    "# get number of samples from each category\n",
    "def get_sample_counts(data_dict):\n",
    "    summary_df = pd.DataFrame(columns=['label','subject'])\n",
    "    for subject in data_dict.keys():\n",
    "        categories = data_dict[subject].keys()\n",
    "        for category in categories:\n",
    "            data = data_dict[subject][category]\n",
    "            nsamples = data.shape[0]\n",
    "            curr_df = pd.DataFrame([category]*nsamples,columns=['label'])\n",
    "            curr_df['subject'] = subject\n",
    "            summary_df = pd.concat((summary_df,curr_df),0)\n",
    "    return summary_df\n",
    "\n",
    "# calculate final number of samples to obtain a uniform distribution across the classes\n",
    "def get_subsampled_sample_counts(summary_df,labels=[0,1,2]):\n",
    "    new_summary_df = pd.DataFrame()\n",
    "    min_nsamples = summary_df['label'].value_counts().min()\n",
    "    for category in labels:\n",
    "        category_df = summary_df[summary_df['label']==category]\n",
    "        subsampled_category_df = category_df.sample(min_nsamples,random_state=0)\n",
    "        new_summary_df = pd.concat((new_summary_df,subsampled_category_df),0)\n",
    "    counts_df = new_summary_df.groupby(by=['subject'])['label'].value_counts()\n",
    "    counts_df.name = 'count'\n",
    "    counts_df = counts_df.reset_index()\n",
    "    return new_summary_df, counts_df\n",
    "\n",
    "def subsample_data_dict(data_dict,counts_df):\n",
    "    #get_class_counts(data_dict)\n",
    "    # subsample data according to above calculated sample numbers\n",
    "    new_data_dict = dict()\n",
    "    for subject in data_dict.keys():\n",
    "        new_data_dict[subject] = dict()\n",
    "        for category in data_dict[subject].keys():\n",
    "            combined_bool = (counts_df['subject']==subject) & (counts_df['label']==category)\n",
    "            if combined_bool.sum() == 0:\n",
    "                continue\n",
    "            count = counts_df[combined_bool]['count'].item()\n",
    "            total_count = data_dict[subject][category].shape[0]\n",
    "            random_indices = random.sample(list(range(total_count)),count)\n",
    "            subsampled_data = data_dict[subject][category][random_indices] \n",
    "            new_data_dict[subject][category] = subsampled_data\n",
    "    #get_class_counts(new_data_dict)\n",
    "    # remove any empty entries\n",
    "    subjects_to_keep = []\n",
    "    for subject,data in new_data_dict.items():\n",
    "        if data != dict():\n",
    "            subjects_to_keep.extend([subject])\n",
    "    new_data_dict = {subject:new_data_dict[subject] for subject in subjects_to_keep}\n",
    "    return new_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4347ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:42.268055Z",
     "iopub.status.busy": "2023-04-05T17:02:42.267341Z",
     "iopub.status.idle": "2023-04-05T17:02:42.274203Z",
     "shell.execute_reply": "2023-04-05T17:02:42.273307Z"
    },
    "papermill": {
     "duration": 0.014457,
     "end_time": "2023-04-05T17:02:42.276317",
     "exception": false,
     "start_time": "2023-04-05T17:02:42.261860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "\n",
    "# if DATASOURCE == 'lab':\n",
    "#     DATA_DIR = os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog')\n",
    "#     tdcsfog_metadata_df = pd.read_csv(os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/tdcsfog_metadata.csv'))\n",
    "#     FILENAMES = [os.path.join(DATA_DIR,file) for file in os.listdir(DATA_DIR) if '.csv' in file]\n",
    "\n",
    "#     df = get_multiclass_df(FILENAMES,tdcsfog_metadata_df,DATASOURCE)\n",
    "    \n",
    "#     multiclass_data_dict = get_data_dict(df)\n",
    "#     multiclass_summary_df = get_sample_counts(multiclass_data_dict)\n",
    "    \n",
    "# elif DATASOURCE == 'realworld':\n",
    "#     DATA_DIR = os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/train/defog')\n",
    "#     defog_metadata_df = pd.read_csv(os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/defog_metadata.csv'))\n",
    "#     FILENAMES = [os.path.join(DATA_DIR,file) for file in os.listdir(DATA_DIR) if '.csv' in file]\n",
    "\n",
    "#     df = get_multiclass_df(FILENAMES,defog_metadata_df,DATASOURCE)\n",
    "    \n",
    "#     multiclass_data_dict = get_data_dict(df,unitConversion=9.81)\n",
    "#     multiclass_summary_df = get_sample_counts(multiclass_data_dict)\n",
    "\n",
    "# elif DATASOURCE == 'lab_and_realworld':\n",
    "#     # LAB data\n",
    "#     DATA_DIR = os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog')\n",
    "#     tdcsfog_metadata_df = pd.read_csv(os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/tdcsfog_metadata.csv'))\n",
    "#     FILENAMES = [os.path.join(DATA_DIR,file) for file in os.listdir(DATA_DIR) if '.csv' in file]\n",
    "\n",
    "#     df = get_multiclass_df(FILENAMES,tdcsfog_metadata_df,'lab')\n",
    "    \n",
    "#     tdcsfog_multiclass_data_dict = get_data_dict(df)\n",
    "#     tdcsfog_multiclass_summary_df = get_sample_counts(tdcsfog_multiclass_data_dict)\n",
    "    \n",
    "#     # REALWORLD data\n",
    "#     DATA_DIR = os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/train/defog')\n",
    "#     defog_metadata_df = pd.read_csv(os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/defog_metadata.csv'))\n",
    "#     FILENAMES = [os.path.join(DATA_DIR,file) for file in os.listdir(DATA_DIR) if '.csv' in file]\n",
    "\n",
    "#     df = get_multiclass_df(FILENAMES,defog_metadata_df,'realworld')\n",
    "    \n",
    "#     defog_multiclass_data_dict = get_data_dict(df,unitConversion=9.81)\n",
    "#     defog_multiclass_summary_df = get_sample_counts(defog_multiclass_data_dict)\n",
    "    \n",
    "#     multiclass_data_dict = {**tdcsfog_multiclass_data_dict,**defog_multiclass_data_dict}\n",
    "#     multiclass_summary_df = pd.concat((tdcsfog_multiclass_summary_df,defog_multiclass_summary_df),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61769fd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:42.286418Z",
     "iopub.status.busy": "2023-04-05T17:02:42.285618Z",
     "iopub.status.idle": "2023-04-05T17:02:42.290790Z",
     "shell.execute_reply": "2023-04-05T17:02:42.289976Z"
    },
    "papermill": {
     "duration": 0.01242,
     "end_time": "2023-04-05T17:02:42.292966",
     "exception": false,
     "start_time": "2023-04-05T17:02:42.280546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "# \"\"\" Subsample the classes \"\"\"\n",
    "# subsampled_multiclass_summary_df, subsampled_multiclass_counts_df = get_subsampled_sample_counts(multiclass_summary_df)\n",
    "# subsampled_multiclass_data_dict = subsample_data_dict(multiclass_data_dict, subsampled_multiclass_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e7d46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:42.302934Z",
     "iopub.status.busy": "2023-04-05T17:02:42.302619Z",
     "iopub.status.idle": "2023-04-05T17:02:42.308407Z",
     "shell.execute_reply": "2023-04-05T17:02:42.307186Z"
    },
    "papermill": {
     "duration": 0.013222,
     "end_time": "2023-04-05T17:02:42.310443",
     "exception": false,
     "start_time": "2023-04-05T17:02:42.297221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "# if TASK == 'binary': # convert problem to binary classification\n",
    "#     \"\"\" Get background data (from lab only for now) \"\"\"\n",
    "#     if DATASOURCE == 'lab':\n",
    "#         metadata_df = tdcsfog_metadata_df\n",
    "#     elif DATASOURCE == 'realworld':\n",
    "#         metadata_df = defog_metadata_df\n",
    "#     background_df = get_background_df(FILENAMES,metadata_df,DATASOURCE)\n",
    "#     background_data_dict = get_data_dict(background_df)\n",
    "    \n",
    "#     # prepare data dict for binary classification (event vs. no event)\n",
    "#     background_summary_df = get_sample_counts(background_data_dict)\n",
    "#     multiclass_summary_df['label'] = multiclass_summary_df['label'].replace({0:1,1:1,2:1})\n",
    "#     background_summary_df['label'] = 0\n",
    "#     binary_summary_df = pd.concat((background_summary_df,multiclass_summary_df),0)\n",
    "#     subsampled_binary_summary_df, subsampled_binary_counts_df = get_subsampled_sample_counts(binary_summary_df,labels=[0,1])\n",
    "    \n",
    "#     # add the background data to a combined data dict\n",
    "#     binary_data_dict = copy.deepcopy(background_data_dict) # more complete list of event series\n",
    "#     for subject in binary_data_dict.keys():\n",
    "#         if subject in multiclass_data_dict:\n",
    "#             binary_data_dict[subject][1] = np.vstack([multiclass_data_dict[subject][cat] for cat in multiclass_data_dict[subject].keys()]) # background originally labelled as -1 (to avoid overlapping with other classes)\n",
    "#         binary_data_dict[subject][0] = binary_data_dict[subject][-1]\n",
    "    \n",
    "#     new_binary_data_dict = dict()\n",
    "#     for subject in binary_data_dict.keys():\n",
    "#         new_binary_data_dict[subject] = dict()\n",
    "#         categories = binary_data_dict[subject].keys()\n",
    "#         for category in categories:\n",
    "#             if category in [0,1]:\n",
    "#                 new_binary_data_dict[subject][category] = binary_data_dict[subject][category]\n",
    "    \n",
    "#     # need to get combine_dict (combine multiclass and background dict)\n",
    "#     subsampled_binary_data_dict = subsample_data_dict(new_binary_data_dict, subsampled_binary_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e07488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:42.321908Z",
     "iopub.status.busy": "2023-04-05T17:02:42.321143Z",
     "iopub.status.idle": "2023-04-05T17:02:42.325647Z",
     "shell.execute_reply": "2023-04-05T17:02:42.324543Z"
    },
    "papermill": {
     "duration": 0.012666,
     "end_time": "2023-04-05T17:02:42.327714",
     "exception": false,
     "start_time": "2023-04-05T17:02:42.315048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "# if TASK == 'multiclass':\n",
    "#     with open('balanced_multiclass_data_dict','wb') as f:\n",
    "#         pickle.dump(subsampled_multiclass_data_dict,f)\n",
    "# elif TASK == 'binary':\n",
    "#     with open('balanced_binary_data_dict','wb') as f:\n",
    "#         pickle.dump(subsampled_binary_data_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e746e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:42.337725Z",
     "iopub.status.busy": "2023-04-05T17:02:42.337462Z",
     "iopub.status.idle": "2023-04-05T17:02:42.346427Z",
     "shell.execute_reply": "2023-04-05T17:02:42.345427Z"
    },
    "papermill": {
     "duration": 0.016092,
     "end_time": "2023-04-05T17:02:42.348474",
     "exception": false,
     "start_time": "2023-04-05T17:02:42.332382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "\"\"\" inspect number of samples from each class \"\"\"\n",
    "def get_class_counts(multiclass_data_dict):\n",
    "    counts = {i:0 for i in [-1,0,1,2]}\n",
    "    for key in multiclass_data_dict.keys():\n",
    "        for cat in multiclass_data_dict[key].keys():\n",
    "            counts[cat] += multiclass_data_dict[key][cat].shape[0]\n",
    "    print(counts)\n",
    "\n",
    "#%%\n",
    "def data_generator(subjects,data_dict,SAMPLES,DATASOURCE):\n",
    "    #random.shuffle(subjects)\n",
    "    for subject in subjects:\n",
    "        #subject = subject.decode(\"utf-8\") # tf encodes input string to utf-8 (therefore you must decode it)\n",
    "        #assert isinstance(data_dict,dict)\n",
    "        if subject in data_dict:\n",
    "            categories_dict = data_dict[subject] \n",
    "            for category in categories_dict.keys():\n",
    "                data = categories_dict[category]\n",
    "                if isinstance(data,np.ndarray):\n",
    "                    nchunks = data.shape[0]\n",
    "                    for i in range(nchunks):\n",
    "                        input_data = categories_dict[category][i] # 256 x 3\n",
    "                        #channel_mean = np.mean(input_data,axis=0)\n",
    "                        #channel_std = np.std(input_data,axis=0)\n",
    "                        #input_data = (input_data - channel_mean)/channel_std\n",
    "                        b,a = signal.butter(2, 15, 'low', fs=128 if DATASOURCE == 'lab' else 100)\n",
    "                        input_data = signal.lfilter(b,a,input_data,axis=0)\n",
    "                        output_data = [category]*SAMPLES \n",
    "                        yield tf.constant(input_data), tf.constant(output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a72644",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:42.358024Z",
     "iopub.status.busy": "2023-04-05T17:02:42.357771Z",
     "iopub.status.idle": "2023-04-05T17:02:42.363842Z",
     "shell.execute_reply": "2023-04-05T17:02:42.362847Z"
    },
    "papermill": {
     "duration": 0.013107,
     "end_time": "2023-04-05T17:02:42.365846",
     "exception": false,
     "start_time": "2023-04-05T17:02:42.352739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_dict(DATASOURCE,TASK):\n",
    "    if TASK == 'multiclass':\n",
    "        if DATASOURCE == 'lab':\n",
    "            folder = 'tdcsfog-multiclass'\n",
    "        with open('/kaggle/input/%s/balanced_multiclass_data_dict' % folder,'rb') as f:\n",
    "            data_dict = pickle.load(f)\n",
    "    elif TASK == 'binary':\n",
    "        if DATASOURCE == 'lab':\n",
    "            folder = 'tcdsfog-binary'\n",
    "        elif DATASOURCE == 'realworld':\n",
    "            folder = 'defog-binary'\n",
    "        with open('/kaggle/input/%s/balanced_binary_data_dict' % folder,'rb') as f:\n",
    "            data_dict = pickle.load(f)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5551b05f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:42.375887Z",
     "iopub.status.busy": "2023-04-05T17:02:42.375614Z",
     "iopub.status.idle": "2023-04-05T17:02:48.163535Z",
     "shell.execute_reply": "2023-04-05T17:02:48.162507Z"
    },
    "papermill": {
     "duration": 5.796393,
     "end_time": "2023-04-05T17:02:48.166471",
     "exception": false,
     "start_time": "2023-04-05T17:02:42.370078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'realworld': {\n",
    "        'binary': {\n",
    "            'epochs': 1, # 5\n",
    "            'batch_size': 128,\n",
    "            'learning_rate': 1e-3,\n",
    "            'folds': 1,\n",
    "            'splits':[0.6,0.2,0.2],\n",
    "            'model': tf.keras.models.Sequential([ \n",
    "                tf.keras.layers.InputLayer(input_shape=(150, 3)),\n",
    "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), # returns output at each time-step (i.e., many to many setup)\n",
    "                tf.keras.layers.Dense(units=1)\n",
    "            ])\n",
    "        }\n",
    "    },\n",
    "    'lab': {\n",
    "        'binary': {\n",
    "            'epochs': 1, # 50 = best\n",
    "            'batch_size': 128,\n",
    "            'learning_rate': 1e-3,\n",
    "            'folds': 1,\n",
    "            'splits':[0.6,0.2,0.2],\n",
    "            'model': tf.keras.models.Sequential([ \n",
    "                            tf.keras.layers.InputLayer(input_shape=(192, 3)),\n",
    "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), # returns output at each time-step (i.e., many to many setup)\n",
    "                            tf.keras.layers.Dense(units=1)\n",
    "                        ])\n",
    "        },\n",
    "        'multiclass': {\n",
    "            'epochs': 1, # 25\n",
    "            'batch_size': 16,\n",
    "            'learning_rate': 1e-4,\n",
    "            'folds': 1,\n",
    "            'splits':[0.8,0.1,0.1],\n",
    "            'model': tf.keras.models.Sequential([ \n",
    "                            tf.keras.layers.InputLayer(input_shape=(192, 3)),\n",
    "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)), # returns output at each time-step (i.e., many to many setup)\n",
    "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)), # returns output at each time-step (i.e., many to many setup)\n",
    "                            tf.keras.layers.Dense(units=3)\n",
    "                        ])\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46367d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:48.177018Z",
     "iopub.status.busy": "2023-04-05T17:02:48.176691Z",
     "iopub.status.idle": "2023-04-05T17:02:48.182960Z",
     "shell.execute_reply": "2023-04-05T17:02:48.181899Z"
    },
    "papermill": {
     "duration": 0.014097,
     "end_time": "2023-04-05T17:02:48.185267",
     "exception": false,
     "start_time": "2023-04-05T17:02:48.171170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class multiclassAUPRC(tf.keras.metrics.AUC):\n",
    "\n",
    "    def __init__(self,**kwargs): # you need to have the kwargs here to be able to load it in later\n",
    "        super(multiclassAUPRC,self).__init__(from_logits=True,curve='PR')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.one_hot(y_true,depth=3)\n",
    "        super().update_state(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13c1fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:02:48.195537Z",
     "iopub.status.busy": "2023-04-05T17:02:48.194941Z",
     "iopub.status.idle": "2023-04-05T17:06:56.284662Z",
     "shell.execute_reply": "2023-04-05T17:06:56.283553Z"
    },
    "papermill": {
     "duration": 248.098127,
     "end_time": "2023-04-05T17:06:56.287611",
     "exception": false,
     "start_time": "2023-04-05T17:02:48.189484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:101: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:111: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Class Distribution...\n",
      "{-1: 0, 0: 32963, 1: 32963, 2: 0}\n",
      "Validation Class Distribution...\n",
      "{-1: 0, 0: 9469, 1: 9469, 2: 0}\n",
      "516/516 [==============================] - 108s 167ms/step - loss: 0.6215 - accuracy: 0.6321 - auc: 0.6697 - val_loss: 0.5962 - val_accuracy: 0.5796 - val_auc: 0.6596\n",
      "Training Class Distribution...\n",
      "{-1: 0, 0: 3419, 1: 3419, 2: 3419}\n",
      "Validation Class Distribution...\n",
      "{-1: 0, 0: 353, 1: 353, 2: 353}\n",
      "642/642 [==============================] - 44s 54ms/step - loss: 1.0681 - accuracy: 0.4719 - multiclass_auprc: 0.4593 - val_loss: 1.0575 - val_accuracy: 0.4339 - val_multiclass_auprc: 0.4913\n",
      "Training Class Distribution...\n",
      "{-1: 0, 0: 9335, 1: 9335, 2: 0}\n",
      "Validation Class Distribution...\n",
      "{-1: 0, 0: 3259, 1: 3259, 2: 0}\n",
      "146/146 [==============================] - 33s 111ms/step - loss: 0.1177 - accuracy: 0.9555 - auc_1: 0.9643 - val_loss: 0.0050 - val_accuracy: 1.0000 - val_auc_1: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "for DATASOURCE in ['lab','realworld']:\n",
    "    if DATASOURCE == 'lab':\n",
    "        TASKS = ['binary','multiclass']\n",
    "    elif DATASOURCE == 'realworld':\n",
    "        TASKS = ['binary']\n",
    "    for TASK in TASKS:\n",
    "        if TASK == 'multiclass':\n",
    "            labels = [0,1,2]\n",
    "        elif TASK == 'binary':\n",
    "            labels = [0,1]\n",
    "        data_dict = load_data_dict(DATASOURCE,TASK)\n",
    "        for fold in range(config[DATASOURCE][TASK]['folds']):  \n",
    "            savefolder = 'defog' if DATASOURCE == 'realworld' else 'tdcsfog'\n",
    "            savetask = TASK\n",
    "            savepath = os.path.join('/kaggle/working',savefolder,savetask)\n",
    "            SAMPLES = 150 if 'realworld' in DATASOURCE else 192\n",
    "\n",
    "            random.seed(fold)\n",
    "            subjects = list(data_dict.keys())\n",
    "            random.shuffle(subjects)\n",
    "            nsubjects = len(subjects)\n",
    "            train_frac, val_frac, test_frac = config[DATASOURCE][TASK]['splits']\n",
    "            train_nsubjects, val_nsubjects = int(train_frac*nsubjects), int(val_frac*nsubjects)\n",
    "            train_subjects, val_subjects, test_subjects = subjects[:train_nsubjects], subjects[train_nsubjects:train_nsubjects+val_nsubjects], subjects[train_nsubjects+val_nsubjects:] \n",
    "\n",
    "            train_data_dict = {subject:data_dict[subject] for subject in train_subjects}\n",
    "            summary_df = get_sample_counts(train_data_dict)\n",
    "            subsampled_summary_df, subsampled_counts_df = get_subsampled_sample_counts(summary_df,labels=labels)\n",
    "            train_data_dict = subsample_data_dict(train_data_dict, subsampled_counts_df)\n",
    "\n",
    "            val_data_dict = {subject:data_dict[subject] for subject in val_subjects}\n",
    "            summary_df = get_sample_counts(val_data_dict)\n",
    "            subsampled_summary_df, subsampled_counts_df = get_subsampled_sample_counts(summary_df,labels=labels)\n",
    "            val_data_dict = subsample_data_dict(val_data_dict, subsampled_counts_df)\n",
    "\n",
    "            test_data_dict = {subject:data_dict[subject] for subject in test_subjects}\n",
    "            summary_df = get_sample_counts(test_data_dict)\n",
    "            subsampled_summary_df, subsampled_counts_df = get_subsampled_sample_counts(summary_df,labels=labels)\n",
    "            test_data_dict = subsample_data_dict(test_data_dict, subsampled_counts_df)\n",
    "\n",
    "            train_data = tf.data.Dataset.from_generator(lambda: data_generator(train_subjects,train_data_dict,SAMPLES,DATASOURCE), # args=[x,y,z]\n",
    "                                                        output_signature=(\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES,3), dtype=tf.float64),\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES), dtype=tf.int32))\n",
    "                                                       ) # shape is at the individual tensor level (not batch)\n",
    "\n",
    "            val_data = tf.data.Dataset.from_generator(lambda: data_generator(val_subjects,val_data_dict,SAMPLES,DATASOURCE),\n",
    "                                                        output_signature=(\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES,3), dtype=tf.float64),\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES), dtype=tf.int32))\n",
    "                                                       ) # shape is at the individual tensor level (not batch)\n",
    "\n",
    "            test_data = tf.data.Dataset.from_generator(lambda: data_generator(test_subjects,test_data_dict,SAMPLES,DATASOURCE),\n",
    "                                                        output_signature=(\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES,3), dtype=tf.float64),\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES), dtype=tf.int32))\n",
    "                                                       )\n",
    "\n",
    "            print('Training Class Distribution...')\n",
    "            get_class_counts(train_data_dict)\n",
    "            print('Validation Class Distribution...')\n",
    "            get_class_counts(val_data_dict)\n",
    "\n",
    "            train_data = train_data.batch(batch_size=config[DATASOURCE][TASK]['batch_size']) # 16\n",
    "            train_data = train_data.shuffle(100,seed=fold)\n",
    "\n",
    "            val_data = val_data.batch(batch_size=128) # 8\n",
    "            test_data = test_data.batch(batch_size=128)\n",
    "\n",
    "            lstm_model = config[DATASOURCE][TASK]['model'] \n",
    "\n",
    "            if TASK == 'multiclass':\n",
    "                lstm_model.compile(\n",
    "                              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                              metrics=['accuracy',multiclassAUPRC()])\n",
    "            elif TASK == 'binary':\n",
    "                lstm_model.compile(\n",
    "                              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                              metrics=['accuracy',tf.keras.metrics.AUC()])\n",
    "\n",
    "            lstm_model.fit(\n",
    "                x = train_data,\n",
    "                validation_data = val_data,\n",
    "                epochs = config[DATASOURCE][TASK]['epochs'],\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0.001,patience=5),\n",
    "                    tf.keras.callbacks.TensorBoard('./logs', update_freq=1),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(\n",
    "                                        filepath=os.path.join(savepath,'checkpoint_fold%i' % fold),\n",
    "                                        save_weights_only=True,\n",
    "                                        monitor='val_accuracy',\n",
    "                                        mode='max',\n",
    "                                        save_best_only=True)\n",
    "                                                ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c854736e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:06:56.394186Z",
     "iopub.status.busy": "2023-04-05T17:06:56.393826Z",
     "iopub.status.idle": "2023-04-05T17:06:56.398570Z",
     "shell.execute_reply": "2023-04-05T17:06:56.397531Z"
    },
    "papermill": {
     "duration": 0.060015,
     "end_time": "2023-04-05T17:06:56.400762",
     "exception": false,
     "start_time": "2023-04-05T17:06:56.340747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lstm_model.load_weights('./tmp/checkpoint_fold0')\n",
    "#lstm_model.evaluate(test_data)\n",
    "#%%\n",
    "#lstm_model.save(os.path.join(SAVE_DIR,'lstm_binary_parkinsons'))\n",
    "#%%\n",
    "#lstm_model = tf.keras.models.load_model(os.path.join(SAVE_DIR,'lstm_parkinsons'),custom_objects={\"multiclassAUPRC\":multiclassAUPRC})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d8c98c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:06:56.504464Z",
     "iopub.status.busy": "2023-04-05T17:06:56.504082Z",
     "iopub.status.idle": "2023-04-05T17:07:06.276010Z",
     "shell.execute_reply": "2023-04-05T17:07:06.274768Z"
    },
    "papermill": {
     "duration": 9.827547,
     "end_time": "2023-04-05T17:07:06.278906",
     "exception": false,
     "start_time": "2023-04-05T17:06:56.451359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 687ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "59/59 [==============================] - 1s 8ms/step\n",
      "46/46 [==============================] - 1s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "# #%%\n",
    "import glob\n",
    "labpaths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog/*.csv\")\n",
    "realpaths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog/*.csv\")\n",
    "test_paths = labpaths + realpaths\n",
    "from collections import OrderedDict \n",
    "\n",
    "p = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/'\n",
    "sub = pd.read_csv(p+'sample_submission.csv')\n",
    "sub['t'] = 0\n",
    "#submission = pd.merge(sub[['Id','t']], all_preds_df, how='left', on='Id').fillna(0.0)\n",
    "\n",
    "all_preds_df = pd.DataFrame()\n",
    "dict_preds_df = OrderedDict()\n",
    "for f in test_paths:\n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    df.set_index('Time', drop=True, inplace=True)\n",
    "    df['Id'] = f.split('/')[-1].split('.')[0]\n",
    "    df['Id'] = df['Id'].astype(str) + '_' + df.index.astype(str)\n",
    "    \n",
    "    curr_sub = pd.merge(sub['Id'],df,how='right',on='Id').fillna(0.0)\n",
    "    \n",
    "    for task in ['binary','multiclass']: #binary',\n",
    "        curr_preds_df = pd.DataFrame()\n",
    "        if 'tdcsfog' in f:\n",
    "            model = config['lab'][task]['model']\n",
    "            model.load_weights(os.path.join('/kaggle/working','tdcsfog',task,'checkpoint_fold0'))\n",
    "            #model = copy.deepcopy(lstm_model)\n",
    "            SAMPLES, jump = 192, 38\n",
    "            unitConversion = 1\n",
    "        elif 'defog' in f:\n",
    "            unitConversion = 9.81\n",
    "            if task == 'binary':\n",
    "                model = config['realworld'][task]['model']\n",
    "                model.load_weights(os.path.join('/kaggle/working','defog',task,'checkpoint_fold0'))\n",
    "                #model = copy.deepcopy(lstm_model)\n",
    "                SAMPLES, jump = 150, 30\n",
    "            elif task == 'multiclass': #use the tdcsfog (lab) model for this case (might not be ideal)\n",
    "                model = config['lab'][task]['model']\n",
    "                model.load_weights(os.path.join('/kaggle/working','tdcsfog',task,'checkpoint_fold0'))\n",
    "                #model = copy.deepcopy(lstm_model)\n",
    "                SAMPLES, jump = 192, 38\n",
    "\n",
    "        if task == 'binary':\n",
    "            CATEGORIES = ['Event']\n",
    "        elif task == 'multiclass':\n",
    "            CATEGORIES = ['StartHesitation','Turn','Walking'] # 3 classes\n",
    "    \n",
    "        start = 0\n",
    "        end = start + SAMPLES\n",
    "        ids_list = []\n",
    "        chunks_list = []\n",
    "        while end <= df.shape[0]:\n",
    "            chunk_df = df.iloc[start:end,:]\n",
    "            chunk_arr = np.array(chunk_df[FEATNAMES]) # SAMPLES x NFEATS\n",
    "            chunk_arr = chunk_arr * unitConversion # conversion first and then filtering\n",
    "            b,a = signal.butter(2, 15, 'low', fs=128 if 'tdcsfog' in f else 100)\n",
    "            chunk_arr = signal.lfilter(b,a,chunk_arr,axis=0)\n",
    "            chunk_arr = np.expand_dims(chunk_arr,0) # 1 x SAMPLES x NFEATS\n",
    "            chunks_list.append(chunk_arr)\n",
    "            ids_list.append(chunk_df['Id'].tolist())\n",
    "            start = start + SAMPLES \n",
    "            end = start + SAMPLES\n",
    "\n",
    "        all_chunks = np.vstack(chunks_list)\n",
    "        preds = model.predict(all_chunks) # B x SAMPLES x NCLASSES\n",
    "        preds_df = pd.DataFrame(preds.reshape(-1,len(CATEGORIES)),columns=CATEGORIES)\n",
    "        preds_df['Id'] = np.concatenate(ids_list)\n",
    "        count = preds_df.shape[0] # number of timestemps thus far\n",
    "        curr_sub = pd.merge(curr_sub['Id'],preds_df,how='left',on='Id').fillna(0.0)\n",
    "        #curr_preds_df = pd.concat((curr_preds_df,preds_df),0)\n",
    "\n",
    "#         # make sure to cover the final (smaller batch)\n",
    "#         final_nsamples = df.shape[0] - count\n",
    "#         if final_nsamples != 0: # there are more timesteps to cover\n",
    "#             chunk_df = df.iloc[-SAMPLES:,:] # because model expects SAMPLES-length input \n",
    "#             chunk_arr = np.array(chunk_df[FEATNAMES]) # SAMPLES x NFEATS\n",
    "#             b,a = signal.butter(2, 15, 'low', fs=128 if 'tdcsfog' in f else 100)\n",
    "#             chunk_arr = signal.lfilter(b,a,chunk_arr,axis=0)\n",
    "#             chunk_arr = chunk_arr * unitConversion\n",
    "#             chunk_arr = np.expand_dims(chunk_arr,0) # 1 x SAMPLES x NFEATS\n",
    "#             preds = model.predict(chunk_arr)\n",
    "#             preds_df = pd.DataFrame(preds.reshape(-1,len(CATEGORIES)),columns=CATEGORIES)\n",
    "#             preds_df = preds_df[-final_nsamples:]\n",
    "#             curr_preds_df = pd.concat((curr_preds_df,preds_df),0)\n",
    "#         curr_preds_df.index = df.index\n",
    "#         curr_preds_df['Id'] = df['Id']\n",
    "        \n",
    "        setting = 'defog' if 'defog' in f else 'tdcsfog'\n",
    "        filename = f.split('/')[-1].split('.')[0]\n",
    "        key = filename + '-' + setting + '-' + task\n",
    "        dict_preds_df[key] = curr_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54ef8737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:07:06.396813Z",
     "iopub.status.busy": "2023-04-05T17:07:06.395753Z",
     "iopub.status.idle": "2023-04-05T17:07:06.572866Z",
     "shell.execute_reply": "2023-04-05T17:07:06.571521Z"
    },
    "papermill": {
     "duration": 0.238051,
     "end_time": "2023-04-05T17:07:06.576314",
     "exception": false,
     "start_time": "2023-04-05T17:07:06.338263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # post processing the binary and multiclass predictions\n",
    "all_preds = []\n",
    "for key in dict_preds_df.keys():\n",
    "    filename,setting,task = key.split('-')\n",
    "    if task == 'multiclass':\n",
    "        binary_preds_df = dict_preds_df[key.replace('multiclass','binary')]\n",
    "        multi_preds_df = dict_preds_df[key]\n",
    "        preds_df = pd.merge(multi_preds_df,binary_preds_df,how='left',on='Id')\n",
    "        preds_df['event_prediction'] = preds_df['Event'] > 0\n",
    "        preds_df.loc[preds_df['event_prediction'] == False,['StartHesitation','Turn','Walking']] = 0.0\n",
    "        preds_df = preds_df[['Id','StartHesitation','Turn','Walking']]\n",
    "        all_preds.append(preds_df)\n",
    "all_preds_df = pd.concat(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f026da7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:07:06.693372Z",
     "iopub.status.busy": "2023-04-05T17:07:06.693000Z",
     "iopub.status.idle": "2023-04-05T17:07:06.697628Z",
     "shell.execute_reply": "2023-04-05T17:07:06.696587Z"
    },
    "papermill": {
     "duration": 0.063329,
     "end_time": "2023-04-05T17:07:06.699797",
     "exception": false,
     "start_time": "2023-04-05T17:07:06.636468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labpaths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog/*.csv\")\n",
    "# realpaths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog/*.csv\")\n",
    "# test_paths = labpaths + realpaths\n",
    "# SAVE_DIR = '/kaggle/working/'\n",
    "# submission = []\n",
    "# for f in test_paths:\n",
    "#     df = pd.read_csv(f)\n",
    "#     df.set_index('Time', drop=True, inplace=True)\n",
    "#     df['Id'] = f.split('/')[-1].split('.')[0]\n",
    "#     df['Id'] = df['Id'].astype(str) + '_' + df.index.astype(str)\n",
    "#     submission.append(df)\n",
    "# submission = pd.concat(submission)\n",
    "# preds_df = pd.DataFrame(submission['Id'])\n",
    "# preds_df[['StartHesitation','Turn','Walking']] = 0\n",
    "#preds_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7097992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:07:06.810597Z",
     "iopub.status.busy": "2023-04-05T17:07:06.810303Z",
     "iopub.status.idle": "2023-04-05T17:07:08.634476Z",
     "shell.execute_reply": "2023-04-05T17:07:08.633381Z"
    },
    "papermill": {
     "duration": 1.882193,
     "end_time": "2023-04-05T17:07:08.637719",
     "exception": false,
     "start_time": "2023-04-05T17:07:06.755526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/'\n",
    "sub = pd.read_csv(p+'sample_submission.csv')\n",
    "sub['t'] = 0\n",
    "submission = pd.merge(sub[['Id','t']], all_preds_df, how='left', on='Id').fillna(0.0)\n",
    "submission[['Id','StartHesitation', 'Turn' , 'Walking']].to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431bc2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T17:07:08.769262Z",
     "iopub.status.busy": "2023-04-05T17:07:08.768865Z",
     "iopub.status.idle": "2023-04-05T17:07:08.773583Z",
     "shell.execute_reply": "2023-04-05T17:07:08.772387Z"
    },
    "papermill": {
     "duration": 0.064343,
     "end_time": "2023-04-05T17:07:08.776314",
     "exception": false,
     "start_time": "2023-04-05T17:07:08.711971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(submission[submission['Id'].str.contains('003f117e14')]['StartHesitation'] == 0).value_counts()\n",
    "#all_preds_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 289.086825,
   "end_time": "2023-04-05T17:07:12.512709",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-05T17:02:23.425884",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
