{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c89d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:19:50.518321Z",
     "iopub.status.busy": "2023-04-04T02:19:50.517669Z",
     "iopub.status.idle": "2023-04-04T02:20:06.371998Z",
     "shell.execute_reply": "2023-04-04T02:20:06.370717Z"
    },
    "papermill": {
     "duration": 15.86538,
     "end_time": "2023-04-04T02:20:06.375896",
     "exception": false,
     "start_time": "2023-04-04T02:19:50.510516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "from scipy import signal\n",
    "import glob\n",
    "\n",
    "CATEGORIES = ['StartHesitation','Turn','Walking'] # 3 classes\n",
    "FEATNAMES = ['AccV','AccML','AccAP']\n",
    "mlb = MultiLabelBinarizer()\n",
    "lenc = LabelEncoder()\n",
    "mlb.fit([CATEGORIES])\n",
    "lenc.fit(CATEGORIES)\n",
    "\n",
    "SOURCE_DIR = '/kaggle/input/'\n",
    "SAVE_DIR = '/kaggle/working/'\n",
    "DATASOURCE = 'realworld' # options: lab | realworld | lab_and_realworld\n",
    "TASK = 'binary' # options: binary | multiclass\n",
    "SAMPLES = 150 # lab = 192, realworld = 150\n",
    "jump = 30 # lab = 38 (80% overlap), realworld = 30\n",
    "\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f1c048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:06.395310Z",
     "iopub.status.busy": "2023-04-04T02:20:06.394568Z",
     "iopub.status.idle": "2023-04-04T02:20:06.441589Z",
     "shell.execute_reply": "2023-04-04T02:20:06.440593Z"
    },
    "papermill": {
     "duration": 0.059587,
     "end_time": "2023-04-04T02:20:06.444537",
     "exception": false,
     "start_time": "2023-04-04T02:20:06.384950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "# populate dataframe only with data associated with labels (0, 1, 2) for the 3 classes\n",
    "def get_multiclass_df(FILENAMES,metadata_df,DATASOURCE):\n",
    "    df = pd.DataFrame()\n",
    "    for filename in tqdm(FILENAMES):\n",
    "        data = pd.read_csv(filename)\n",
    "        if DATASOURCE == 'realworld': # only consider rows with unambiguous annotations\n",
    "            data = data[(data['Valid']==True) & (data['Task']==True)]\n",
    "            data.reset_index(inplace=True,drop=True)\n",
    "        nevents = data[CATEGORIES].sum().sum()\n",
    "        if nevents == 0: # skip if subject has no event whatsoever\n",
    "            continue\n",
    "        labels = mlb.inverse_transform(np.array(data[CATEGORIES])) # returns class tuple\n",
    "        eventId = filename.split('/')[-1].split('.csv')[0]\n",
    "        subjectId = metadata_df[metadata_df['Id']==eventId]['Subject'].item()\n",
    "        subjectId = eventId\n",
    "        \n",
    "        for category in CATEGORIES:\n",
    "            condition = data[category]==1\n",
    "            category_df = data[FEATNAMES][condition]\n",
    "            category_df['label'] = lenc.transform(pd.DataFrame(labels)[condition].iloc[:,0])\n",
    "            category_df['subject'] = subjectId\n",
    "            category_df['series'] = eventId\n",
    "            category_df.reset_index(inplace=True,drop=True)\n",
    "            df = pd.concat((df,category_df),0)\n",
    "\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    return df\n",
    "\n",
    "#%%\n",
    "# obtain samples for the background class\n",
    "#events_df = pd.read_csv('/home/danikiyasseh/datasets/tlvmc-parkinsons-freezing-gait-prediction/events.csv')\n",
    "def get_background_df(FILENAMES,metadata_df,DATASOURCE):\n",
    "    background_df = pd.DataFrame()\n",
    "    for filename in tqdm(FILENAMES):\n",
    "        data = pd.read_csv(filename)\n",
    "        if DATASOURCE == 'realworld': # only consider rows with unambiguous annotations\n",
    "            data = data[(data['Valid']==True) & (data['Task']==True)]\n",
    "            data.reset_index(inplace=True,drop=True)\n",
    "        labels = [-1]*data.shape[0]\n",
    "        eventId = filename.split('/')[-1].split('.csv')[0]\n",
    "        subjectId = metadata_df[metadata_df['Id']==eventId]['Subject'].item()\n",
    "        subjectId = eventId\n",
    "    \n",
    "        condition = ~data[CATEGORIES].any(axis=1)\n",
    "        category_df = data[FEATNAMES][condition]\n",
    "        category_df['label'] = pd.DataFrame(labels)[condition]\n",
    "        category_df['subject'] = subjectId\n",
    "        category_df['series'] = eventId\n",
    "        category_df.reset_index(inplace=True,drop=True)\n",
    "        background_df = pd.concat((background_df,category_df),0)\n",
    "    \n",
    "    background_df['label'] = background_df['label'].astype(int)\n",
    "    return background_df\n",
    "\n",
    "#%%\n",
    "# prepare samples in dictionary format\n",
    "def get_data_dict(df,unitConversion=1):\n",
    "    data_dict = dict()\n",
    "    for subject in tqdm(df['subject'].unique()):\n",
    "        data_dict[subject] = defaultdict(list)\n",
    "        subject_df = df[df['subject']==subject]\n",
    "        for series in subject_df['series'].unique():\n",
    "            series_df = subject_df[subject_df['series']==series]\n",
    "            for category in series_df['label'].unique():\n",
    "                category_df = series_df[series_df['label']==category]\n",
    "                if category_df.shape[0] >= SAMPLES: # at least this many samples for this subject from this category\n",
    "            \n",
    "                    start = 0\n",
    "                    end = start + SAMPLES\n",
    "                    while end <= category_df.shape[0]:\n",
    "                        chunk_category_df = category_df[start:end]\n",
    "                        chunk_category_arr = np.array(chunk_category_df[FEATNAMES]) * unitConversion # SAMPLES x NFEATS\n",
    "                        data_dict[subject][category].append(chunk_category_arr)\n",
    "                        start = start + jump\n",
    "                        end = start + SAMPLES\n",
    "                \n",
    "        for category in subject_df['label'].unique():\n",
    "            if len(data_dict[subject][category]) == 0:\n",
    "                data_dict[subject].pop(category)\n",
    "            else:\n",
    "                data_dict[subject][category] = np.stack(data_dict[subject][category]) # NCHUNKS x SAMPLES x NFEATS\n",
    "    \n",
    "    subjects_to_keep = [] # remove empty entries\n",
    "    for subject,data in data_dict.items():\n",
    "        if data != dict():\n",
    "            subjects_to_keep.extend([subject])\n",
    "    new_data_dict = {subject:data_dict[subject] for subject in subjects_to_keep}\n",
    "    return new_data_dict\n",
    "\n",
    "# get number of samples from each category\n",
    "def get_sample_counts(data_dict):\n",
    "    summary_df = pd.DataFrame(columns=['label','subject'])\n",
    "    for subject in data_dict.keys():\n",
    "        categories = data_dict[subject].keys()\n",
    "        for category in categories:\n",
    "            data = data_dict[subject][category]\n",
    "            nsamples = data.shape[0]\n",
    "            curr_df = pd.DataFrame([category]*nsamples,columns=['label'])\n",
    "            curr_df['subject'] = subject\n",
    "            summary_df = pd.concat((summary_df,curr_df),0)\n",
    "    return summary_df\n",
    "\n",
    "# calculate final number of samples to obtain a uniform distribution across the classes\n",
    "def get_subsampled_sample_counts(summary_df,labels=[0,1,2]):\n",
    "    new_summary_df = pd.DataFrame()\n",
    "    min_nsamples = summary_df['label'].value_counts().min()\n",
    "    for category in labels:\n",
    "        category_df = summary_df[summary_df['label']==category]\n",
    "        subsampled_category_df = category_df.sample(min_nsamples,random_state=0)\n",
    "        new_summary_df = pd.concat((new_summary_df,subsampled_category_df),0)\n",
    "    counts_df = new_summary_df.groupby(by=['subject'])['label'].value_counts()\n",
    "    counts_df.name = 'count'\n",
    "    counts_df = counts_df.reset_index()\n",
    "    return new_summary_df, counts_df\n",
    "\n",
    "def subsample_data_dict(data_dict,counts_df):\n",
    "    #get_class_counts(data_dict)\n",
    "    # subsample data according to above calculated sample numbers\n",
    "    new_data_dict = dict()\n",
    "    for subject in data_dict.keys():\n",
    "        new_data_dict[subject] = dict()\n",
    "        for category in data_dict[subject].keys():\n",
    "            combined_bool = (counts_df['subject']==subject) & (counts_df['label']==category)\n",
    "            if combined_bool.sum() == 0:\n",
    "                continue\n",
    "            count = counts_df[combined_bool]['count'].item()\n",
    "            total_count = data_dict[subject][category].shape[0]\n",
    "            random_indices = random.sample(list(range(total_count)),count)\n",
    "            subsampled_data = data_dict[subject][category][random_indices] \n",
    "            new_data_dict[subject][category] = subsampled_data\n",
    "    #get_class_counts(new_data_dict)\n",
    "    # remove any empty entries\n",
    "    subjects_to_keep = []\n",
    "    for subject,data in new_data_dict.items():\n",
    "        if data != dict():\n",
    "            subjects_to_keep.extend([subject])\n",
    "    new_data_dict = {subject:new_data_dict[subject] for subject in subjects_to_keep}\n",
    "    return new_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8664a2ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:06.462582Z",
     "iopub.status.busy": "2023-04-04T02:20:06.462178Z",
     "iopub.status.idle": "2023-04-04T02:20:06.471205Z",
     "shell.execute_reply": "2023-04-04T02:20:06.470211Z"
    },
    "papermill": {
     "duration": 0.02131,
     "end_time": "2023-04-04T02:20:06.474326",
     "exception": false,
     "start_time": "2023-04-04T02:20:06.453016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "\n",
    "# if DATASOURCE == 'lab':\n",
    "#     DATA_DIR = os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog')\n",
    "#     tdcsfog_metadata_df = pd.read_csv(os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/tdcsfog_metadata.csv'))\n",
    "#     FILENAMES = [os.path.join(DATA_DIR,file) for file in os.listdir(DATA_DIR) if '.csv' in file]\n",
    "\n",
    "#     df = get_multiclass_df(FILENAMES,tdcsfog_metadata_df,DATASOURCE)\n",
    "    \n",
    "#     multiclass_data_dict = get_data_dict(df)\n",
    "#     multiclass_summary_df = get_sample_counts(multiclass_data_dict)\n",
    "    \n",
    "# elif DATASOURCE == 'realworld':\n",
    "#     DATA_DIR = os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/train/defog')\n",
    "#     defog_metadata_df = pd.read_csv(os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/defog_metadata.csv'))\n",
    "#     FILENAMES = [os.path.join(DATA_DIR,file) for file in os.listdir(DATA_DIR) if '.csv' in file]\n",
    "\n",
    "#     df = get_multiclass_df(FILENAMES,defog_metadata_df,DATASOURCE)\n",
    "    \n",
    "#     multiclass_data_dict = get_data_dict(df,unitConversion=9.81)\n",
    "#     multiclass_summary_df = get_sample_counts(multiclass_data_dict)\n",
    "\n",
    "# elif DATASOURCE == 'lab_and_realworld':\n",
    "#     # LAB data\n",
    "#     DATA_DIR = os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog')\n",
    "#     tdcsfog_metadata_df = pd.read_csv(os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/tdcsfog_metadata.csv'))\n",
    "#     FILENAMES = [os.path.join(DATA_DIR,file) for file in os.listdir(DATA_DIR) if '.csv' in file]\n",
    "\n",
    "#     df = get_multiclass_df(FILENAMES,tdcsfog_metadata_df,'lab')\n",
    "    \n",
    "#     tdcsfog_multiclass_data_dict = get_data_dict(df)\n",
    "#     tdcsfog_multiclass_summary_df = get_sample_counts(tdcsfog_multiclass_data_dict)\n",
    "    \n",
    "#     # REALWORLD data\n",
    "#     DATA_DIR = os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/train/defog')\n",
    "#     defog_metadata_df = pd.read_csv(os.path.join(SOURCE_DIR,'tlvmc-parkinsons-freezing-gait-prediction/defog_metadata.csv'))\n",
    "#     FILENAMES = [os.path.join(DATA_DIR,file) for file in os.listdir(DATA_DIR) if '.csv' in file]\n",
    "\n",
    "#     df = get_multiclass_df(FILENAMES,defog_metadata_df,'realworld')\n",
    "    \n",
    "#     defog_multiclass_data_dict = get_data_dict(df,unitConversion=9.81)\n",
    "#     defog_multiclass_summary_df = get_sample_counts(defog_multiclass_data_dict)\n",
    "    \n",
    "#     multiclass_data_dict = {**tdcsfog_multiclass_data_dict,**defog_multiclass_data_dict}\n",
    "#     multiclass_summary_df = pd.concat((tdcsfog_multiclass_summary_df,defog_multiclass_summary_df),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2c3323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:06.491845Z",
     "iopub.status.busy": "2023-04-04T02:20:06.491481Z",
     "iopub.status.idle": "2023-04-04T02:20:06.496029Z",
     "shell.execute_reply": "2023-04-04T02:20:06.495038Z"
    },
    "papermill": {
     "duration": 0.018761,
     "end_time": "2023-04-04T02:20:06.501096",
     "exception": false,
     "start_time": "2023-04-04T02:20:06.482335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "# \"\"\" Subsample the classes \"\"\"\n",
    "# subsampled_multiclass_summary_df, subsampled_multiclass_counts_df = get_subsampled_sample_counts(multiclass_summary_df)\n",
    "# subsampled_multiclass_data_dict = subsample_data_dict(multiclass_data_dict, subsampled_multiclass_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2307c2c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:06.517535Z",
     "iopub.status.busy": "2023-04-04T02:20:06.517178Z",
     "iopub.status.idle": "2023-04-04T02:20:06.523256Z",
     "shell.execute_reply": "2023-04-04T02:20:06.522338Z"
    },
    "papermill": {
     "duration": 0.01889,
     "end_time": "2023-04-04T02:20:06.527763",
     "exception": false,
     "start_time": "2023-04-04T02:20:06.508873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "# if TASK == 'binary': # convert problem to binary classification\n",
    "#     \"\"\" Get background data (from lab only for now) \"\"\"\n",
    "#     if DATASOURCE == 'lab':\n",
    "#         metadata_df = tdcsfog_metadata_df\n",
    "#     elif DATASOURCE == 'realworld':\n",
    "#         metadata_df = defog_metadata_df\n",
    "#     background_df = get_background_df(FILENAMES,metadata_df,DATASOURCE)\n",
    "#     background_data_dict = get_data_dict(background_df)\n",
    "    \n",
    "#     # prepare data dict for binary classification (event vs. no event)\n",
    "#     background_summary_df = get_sample_counts(background_data_dict)\n",
    "#     multiclass_summary_df['label'] = multiclass_summary_df['label'].replace({0:1,1:1,2:1})\n",
    "#     background_summary_df['label'] = 0\n",
    "#     binary_summary_df = pd.concat((background_summary_df,multiclass_summary_df),0)\n",
    "#     subsampled_binary_summary_df, subsampled_binary_counts_df = get_subsampled_sample_counts(binary_summary_df,labels=[0,1])\n",
    "    \n",
    "#     # add the background data to a combined data dict\n",
    "#     binary_data_dict = copy.deepcopy(background_data_dict) # more complete list of event series\n",
    "#     for subject in binary_data_dict.keys():\n",
    "#         if subject in multiclass_data_dict:\n",
    "#             binary_data_dict[subject][1] = np.vstack([multiclass_data_dict[subject][cat] for cat in multiclass_data_dict[subject].keys()]) # background originally labelled as -1 (to avoid overlapping with other classes)\n",
    "#         binary_data_dict[subject][0] = binary_data_dict[subject][-1]\n",
    "    \n",
    "#     new_binary_data_dict = dict()\n",
    "#     for subject in binary_data_dict.keys():\n",
    "#         new_binary_data_dict[subject] = dict()\n",
    "#         categories = binary_data_dict[subject].keys()\n",
    "#         for category in categories:\n",
    "#             if category in [0,1]:\n",
    "#                 new_binary_data_dict[subject][category] = binary_data_dict[subject][category]\n",
    "    \n",
    "#     # need to get combine_dict (combine multiclass and background dict)\n",
    "#     subsampled_binary_data_dict = subsample_data_dict(new_binary_data_dict, subsampled_binary_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f0514c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:06.543780Z",
     "iopub.status.busy": "2023-04-04T02:20:06.543431Z",
     "iopub.status.idle": "2023-04-04T02:20:06.547564Z",
     "shell.execute_reply": "2023-04-04T02:20:06.546673Z"
    },
    "papermill": {
     "duration": 0.016737,
     "end_time": "2023-04-04T02:20:06.551909",
     "exception": false,
     "start_time": "2023-04-04T02:20:06.535172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #%%\n",
    "# if TASK == 'multiclass':\n",
    "#     with open('balanced_multiclass_data_dict','wb') as f:\n",
    "#         pickle.dump(subsampled_multiclass_data_dict,f)\n",
    "# elif TASK == 'binary':\n",
    "#     with open('balanced_binary_data_dict','wb') as f:\n",
    "#         pickle.dump(subsampled_binary_data_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc862ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:06.567352Z",
     "iopub.status.busy": "2023-04-04T02:20:06.567024Z",
     "iopub.status.idle": "2023-04-04T02:20:06.580219Z",
     "shell.execute_reply": "2023-04-04T02:20:06.579343Z"
    },
    "papermill": {
     "duration": 0.023647,
     "end_time": "2023-04-04T02:20:06.582873",
     "exception": false,
     "start_time": "2023-04-04T02:20:06.559226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "\"\"\" inspect number of samples from each class \"\"\"\n",
    "def get_class_counts(multiclass_data_dict):\n",
    "    counts = {i:0 for i in [-1,0,1,2]}\n",
    "    for key in multiclass_data_dict.keys():\n",
    "        for cat in multiclass_data_dict[key].keys():\n",
    "            counts[cat] += multiclass_data_dict[key][cat].shape[0]\n",
    "    print(counts)\n",
    "\n",
    "#%%\n",
    "def data_generator(subjects,data_dict,SAMPLES,DATASOURCE):\n",
    "    #random.shuffle(subjects)\n",
    "    for subject in subjects:\n",
    "        #subject = subject.decode(\"utf-8\") # tf encodes input string to utf-8 (therefore you must decode it)\n",
    "        #assert isinstance(data_dict,dict)\n",
    "        if subject in data_dict:\n",
    "            categories_dict = data_dict[subject] \n",
    "            for category in categories_dict.keys():\n",
    "                data = categories_dict[category]\n",
    "                if isinstance(data,np.ndarray):\n",
    "                    nchunks = data.shape[0]\n",
    "                    for i in range(nchunks):\n",
    "                        input_data = categories_dict[category][i] # 256 x 3\n",
    "                        #channel_mean = np.mean(input_data,axis=0)\n",
    "                        #channel_std = np.std(input_data,axis=0)\n",
    "                        #input_data = (input_data - channel_mean)/channel_std\n",
    "                        b,a = signal.butter(2, 15, 'low', fs=128 if DATASOURCE == 'lab' else 100)\n",
    "                        input_data = signal.lfilter(b,a,input_data,axis=0)\n",
    "                        output_data = [category]*SAMPLES \n",
    "                        yield tf.constant(input_data), tf.constant(output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff10bd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:06.599183Z",
     "iopub.status.busy": "2023-04-04T02:20:06.598872Z",
     "iopub.status.idle": "2023-04-04T02:20:06.607253Z",
     "shell.execute_reply": "2023-04-04T02:20:06.606325Z"
    },
    "papermill": {
     "duration": 0.019909,
     "end_time": "2023-04-04T02:20:06.610199",
     "exception": false,
     "start_time": "2023-04-04T02:20:06.590290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_dict(DATASOURCE,TASK):\n",
    "    if TASK == 'multiclass':\n",
    "        if DATASOURCE == 'lab':\n",
    "            folder = 'tdcsfog-multiclass'\n",
    "        with open('/kaggle/input/%s/balanced_multiclass_data_dict' % folder,'rb') as f:\n",
    "            data_dict = pickle.load(f)\n",
    "    elif TASK == 'binary':\n",
    "        if DATASOURCE == 'lab':\n",
    "            folder = 'tcdsfog-binary'\n",
    "        elif DATASOURCE == 'realworld':\n",
    "            folder = 'defog-binary'\n",
    "        with open('/kaggle/input/%s/balanced_binary_data_dict' % folder,'rb') as f:\n",
    "            data_dict = pickle.load(f)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ff9965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:06.625599Z",
     "iopub.status.busy": "2023-04-04T02:20:06.625258Z",
     "iopub.status.idle": "2023-04-04T02:20:13.813417Z",
     "shell.execute_reply": "2023-04-04T02:20:13.812320Z"
    },
    "papermill": {
     "duration": 7.198571,
     "end_time": "2023-04-04T02:20:13.815905",
     "exception": false,
     "start_time": "2023-04-04T02:20:06.617334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'realworld': {\n",
    "        'binary': {\n",
    "            'epochs': 1, # 5\n",
    "            'batch_size': 128,\n",
    "            'learning_rate': 1e-3,\n",
    "            'folds': 1,\n",
    "            'splits':[0.6,0.2,0.2],\n",
    "            'model': tf.keras.models.Sequential([ \n",
    "                tf.keras.layers.InputLayer(input_shape=(150, 3)),\n",
    "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), # returns output at each time-step (i.e., many to many setup)\n",
    "                tf.keras.layers.Dense(units=1)\n",
    "            ])\n",
    "        }\n",
    "    },\n",
    "    'lab': {\n",
    "        'binary': {\n",
    "            'epochs': 1, # 50 = best\n",
    "            'batch_size': 128,\n",
    "            'learning_rate': 1e-3,\n",
    "            'folds': 1,\n",
    "            'splits':[0.6,0.2,0.2],\n",
    "            'model': tf.keras.models.Sequential([ \n",
    "                            tf.keras.layers.InputLayer(input_shape=(192, 3)),\n",
    "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)), # returns output at each time-step (i.e., many to many setup)\n",
    "                            tf.keras.layers.Dense(units=1)\n",
    "                        ])\n",
    "        },\n",
    "        'multiclass': {\n",
    "            'epochs': 1, # 25\n",
    "            'batch_size': 16,\n",
    "            'learning_rate': 1e-4,\n",
    "            'folds': 1,\n",
    "            'splits':[0.8,0.1,0.1],\n",
    "            'model': tf.keras.models.Sequential([ \n",
    "                            tf.keras.layers.InputLayer(input_shape=(192, 3)),\n",
    "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)), # returns output at each time-step (i.e., many to many setup)\n",
    "                            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)), # returns output at each time-step (i.e., many to many setup)\n",
    "                            tf.keras.layers.Dense(units=3)\n",
    "                        ])\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6464ec43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:13.827819Z",
     "iopub.status.busy": "2023-04-04T02:20:13.827500Z",
     "iopub.status.idle": "2023-04-04T02:20:13.833223Z",
     "shell.execute_reply": "2023-04-04T02:20:13.832225Z"
    },
    "papermill": {
     "duration": 0.014073,
     "end_time": "2023-04-04T02:20:13.835503",
     "exception": false,
     "start_time": "2023-04-04T02:20:13.821430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class multiclassAUPRC(tf.keras.metrics.AUC):\n",
    "\n",
    "    def __init__(self,**kwargs): # you need to have the kwargs here to be able to load it in later\n",
    "        super(multiclassAUPRC,self).__init__(from_logits=True,curve='PR')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.one_hot(y_true,depth=3)\n",
    "        super().update_state(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa52136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:13.846280Z",
     "iopub.status.busy": "2023-04-04T02:20:13.846012Z",
     "iopub.status.idle": "2023-04-04T02:20:13.850380Z",
     "shell.execute_reply": "2023-04-04T02:20:13.849419Z"
    },
    "papermill": {
     "duration": 0.012598,
     "end_time": "2023-04-04T02:20:13.852732",
     "exception": false,
     "start_time": "2023-04-04T02:20:13.840134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preds = model.predict(a).squeeze()\n",
    "#labels = list(val_data.take(1))[0][1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9f77bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:13.863939Z",
     "iopub.status.busy": "2023-04-04T02:20:13.862895Z",
     "iopub.status.idle": "2023-04-04T02:20:13.867465Z",
     "shell.execute_reply": "2023-04-04T02:20:13.866560Z"
    },
    "papermill": {
     "duration": 0.012399,
     "end_time": "2023-04-04T02:20:13.869637",
     "exception": false,
     "start_time": "2023-04-04T02:20:13.857238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# val_data = val_data.shuffle(100,seed=0)\n",
    "# for a,b in val_data.as_numpy_iterator():\n",
    "#     if 0 in b and b.sum() > 32*150:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1cbfae1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:13.880303Z",
     "iopub.status.busy": "2023-04-04T02:20:13.880030Z",
     "iopub.status.idle": "2023-04-04T02:20:13.884905Z",
     "shell.execute_reply": "2023-04-04T02:20:13.884022Z"
    },
    "papermill": {
     "duration": 0.012876,
     "end_time": "2023-04-04T02:20:13.887138",
     "exception": false,
     "start_time": "2023-04-04T02:20:13.874262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preds2 = preds.reshape(-1,1)[:,0]\n",
    "# labels2 = labels.reshape(-1,1)[:,0]\n",
    "# df = pd.DataFrame([preds2,labels2]).T #,columns=['Pred','Label'])\n",
    "# df.columns = ['Pred','Label']\n",
    "# sns.histplot(x='Pred',hue='Label',data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d97d37c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:20:13.898058Z",
     "iopub.status.busy": "2023-04-04T02:20:13.897777Z",
     "iopub.status.idle": "2023-04-04T02:21:44.801456Z",
     "shell.execute_reply": "2023-04-04T02:21:44.800464Z"
    },
    "papermill": {
     "duration": 90.912586,
     "end_time": "2023-04-04T02:21:44.804321",
     "exception": false,
     "start_time": "2023-04-04T02:20:13.891735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:101: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:111: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Class Distribution...\n",
      "{-1: 0, 0: 3419, 1: 3419, 2: 3419}\n",
      "Validation Class Distribution...\n",
      "{-1: 0, 0: 353, 1: 353, 2: 353}\n",
      "642/642 [==============================] - 54s 57ms/step - loss: 1.0739 - accuracy: 0.4610 - multiclass_auprc: 0.4555 - val_loss: 1.0848 - val_accuracy: 0.4186 - val_multiclass_auprc: 0.4549\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "for DATASOURCE in ['lab']:#,'realworld']:\n",
    "    if DATASOURCE == 'lab':\n",
    "        TASKS = ['multiclass'] #['binary','multiclass']\n",
    "    elif DATASOURCE == 'realworld':\n",
    "        TASKS = ['binary']\n",
    "    for TASK in TASKS:\n",
    "        if TASK == 'multiclass':\n",
    "            labels = [0,1,2]\n",
    "        elif TASK == 'binary':\n",
    "            labels = [0,1]\n",
    "        data_dict = load_data_dict(DATASOURCE,TASK)\n",
    "        for fold in range(config[DATASOURCE][TASK]['folds']):  \n",
    "            savefolder = 'defog' if DATASOURCE == 'realworld' else 'tdcsfog'\n",
    "            savetask = TASK\n",
    "            savepath = os.path.join('/kaggle/working',savefolder,savetask)\n",
    "            SAMPLES = 150 if 'realworld' in DATASOURCE else 192\n",
    "\n",
    "            random.seed(fold)\n",
    "            subjects = list(data_dict.keys())\n",
    "            random.shuffle(subjects)\n",
    "            nsubjects = len(subjects)\n",
    "            train_frac, val_frac, test_frac = config[DATASOURCE][TASK]['splits']\n",
    "            train_nsubjects, val_nsubjects = int(train_frac*nsubjects), int(val_frac*nsubjects)\n",
    "            train_subjects, val_subjects, test_subjects = subjects[:train_nsubjects], subjects[train_nsubjects:train_nsubjects+val_nsubjects], subjects[train_nsubjects+val_nsubjects:] \n",
    "\n",
    "            train_data_dict = {subject:data_dict[subject] for subject in train_subjects}\n",
    "            summary_df = get_sample_counts(train_data_dict)\n",
    "            subsampled_summary_df, subsampled_counts_df = get_subsampled_sample_counts(summary_df,labels=labels)\n",
    "            train_data_dict = subsample_data_dict(train_data_dict, subsampled_counts_df)\n",
    "\n",
    "            val_data_dict = {subject:data_dict[subject] for subject in val_subjects}\n",
    "            summary_df = get_sample_counts(val_data_dict)\n",
    "            subsampled_summary_df, subsampled_counts_df = get_subsampled_sample_counts(summary_df,labels=labels)\n",
    "            val_data_dict = subsample_data_dict(val_data_dict, subsampled_counts_df)\n",
    "\n",
    "            test_data_dict = {subject:data_dict[subject] for subject in test_subjects}\n",
    "            summary_df = get_sample_counts(test_data_dict)\n",
    "            subsampled_summary_df, subsampled_counts_df = get_subsampled_sample_counts(summary_df,labels=labels)\n",
    "            test_data_dict = subsample_data_dict(test_data_dict, subsampled_counts_df)\n",
    "\n",
    "            train_data = tf.data.Dataset.from_generator(lambda: data_generator(train_subjects,train_data_dict,SAMPLES,DATASOURCE), # args=[x,y,z]\n",
    "                                                        output_signature=(\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES,3), dtype=tf.float64),\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES), dtype=tf.int32))\n",
    "                                                       ) # shape is at the individual tensor level (not batch)\n",
    "\n",
    "            val_data = tf.data.Dataset.from_generator(lambda: data_generator(val_subjects,val_data_dict,SAMPLES,DATASOURCE),\n",
    "                                                        output_signature=(\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES,3), dtype=tf.float64),\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES), dtype=tf.int32))\n",
    "                                                       ) # shape is at the individual tensor level (not batch)\n",
    "\n",
    "            test_data = tf.data.Dataset.from_generator(lambda: data_generator(test_subjects,test_data_dict,SAMPLES,DATASOURCE),\n",
    "                                                        output_signature=(\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES,3), dtype=tf.float64),\n",
    "                                                           tf.TensorSpec(shape=(SAMPLES), dtype=tf.int32))\n",
    "                                                       )\n",
    "\n",
    "            print('Training Class Distribution...')\n",
    "            get_class_counts(train_data_dict)\n",
    "            print('Validation Class Distribution...')\n",
    "            get_class_counts(val_data_dict)\n",
    "\n",
    "            train_data = train_data.batch(batch_size=config[DATASOURCE][TASK]['batch_size']) # 16\n",
    "            train_data = train_data.shuffle(100,seed=fold)\n",
    "\n",
    "            val_data = val_data.batch(batch_size=128) # 8\n",
    "            test_data = test_data.batch(batch_size=128)\n",
    "\n",
    "            lstm_model = config[DATASOURCE][TASK]['model'] \n",
    "\n",
    "            if TASK == 'multiclass':\n",
    "                lstm_model.compile(\n",
    "                              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                              metrics=['accuracy',multiclassAUPRC()])\n",
    "            elif TASK == 'binary':\n",
    "                lstm_model.compile(\n",
    "                              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                              metrics=['accuracy',tf.keras.metrics.AUC()])\n",
    "\n",
    "            lstm_model.fit(\n",
    "                x = train_data,\n",
    "                validation_data = val_data,\n",
    "                epochs = config[DATASOURCE][TASK]['epochs'],\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0.001,patience=5),\n",
    "                    tf.keras.callbacks.TensorBoard('./logs', update_freq=1),\n",
    "                    tf.keras.callbacks.ModelCheckpoint(\n",
    "                                        filepath=os.path.join(savepath,'checkpoint_fold%i' % fold),\n",
    "                                        save_weights_only=True,\n",
    "                                        monitor='val_accuracy',\n",
    "                                        mode='max',\n",
    "                                        save_best_only=True)\n",
    "                                                ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf42d2b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:21:44.865216Z",
     "iopub.status.busy": "2023-04-04T02:21:44.864281Z",
     "iopub.status.idle": "2023-04-04T02:21:44.869932Z",
     "shell.execute_reply": "2023-04-04T02:21:44.868932Z"
    },
    "papermill": {
     "duration": 0.037938,
     "end_time": "2023-04-04T02:21:44.872301",
     "exception": false,
     "start_time": "2023-04-04T02:21:44.834363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lstm_model.load_weights('./tmp/checkpoint_fold0')\n",
    "#lstm_model.evaluate(test_data)\n",
    "#%%\n",
    "#lstm_model.save(os.path.join(SAVE_DIR,'lstm_binary_parkinsons'))\n",
    "#%%\n",
    "#lstm_model = tf.keras.models.load_model(os.path.join(SAVE_DIR,'lstm_parkinsons'),custom_objects={\"multiclassAUPRC\":multiclassAUPRC})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6219b10",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-04-04T02:21:44.931680Z",
     "iopub.status.busy": "2023-04-04T02:21:44.931240Z",
     "iopub.status.idle": "2023-04-04T02:21:56.490724Z",
     "shell.execute_reply": "2023-04-04T02:21:56.489557Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 11.592445,
     "end_time": "2023-04-04T02:21:56.493329",
     "exception": false,
     "start_time": "2023-04-04T02:21:44.900884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......bidirectional\n",
      ".........backward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........forward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........layer\n",
      "............cell\n",
      "...............vars\n",
      "............vars\n",
      ".........vars\n",
      "......bidirectional_1\n",
      ".........backward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........forward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........layer\n",
      "............cell\n",
      "...............vars\n",
      "............vars\n",
      ".........vars\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......multiclass_auprc\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-04 02:21:44         3868\n",
      "variables.h5                                   2023-04-04 02:21:45     25367944\n",
      "metadata.json                                  2023-04-04 02:21:44           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-04 02:21:44         3868\n",
      "variables.h5                                   2023-04-04 02:21:44     25367944\n",
      "metadata.json                                  2023-04-04 02:21:44           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......bidirectional\n",
      ".........backward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........forward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........layer\n",
      "............cell\n",
      "...............vars\n",
      "............vars\n",
      ".........vars\n",
      "......bidirectional_1\n",
      ".........backward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........forward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........layer\n",
      "............cell\n",
      "...............vars\n",
      "............vars\n",
      ".........vars\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......multiclass_auprc\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:70: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:84: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......bidirectional\n",
      ".........backward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........forward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........layer\n",
      "............cell\n",
      "...............vars\n",
      "............vars\n",
      ".........vars\n",
      "......bidirectional_1\n",
      ".........backward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........forward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........layer\n",
      "............cell\n",
      "...............vars\n",
      "............vars\n",
      ".........vars\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......multiclass_auprc\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-04 02:21:48         3868\n",
      "variables.h5                                   2023-04-04 02:21:48     25367944\n",
      "metadata.json                                  2023-04-04 02:21:48           64\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-04-04 02:21:48         3868\n",
      "variables.h5                                   2023-04-04 02:21:48     25367944\n",
      "metadata.json                                  2023-04-04 02:21:48           64\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......bidirectional\n",
      ".........backward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........forward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........layer\n",
      "............cell\n",
      "...............vars\n",
      "............vars\n",
      ".........vars\n",
      "......bidirectional_1\n",
      ".........backward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........forward_layer\n",
      "............cell\n",
      "...............vars\n",
      "..................0\n",
      "..................1\n",
      "..................2\n",
      "............vars\n",
      ".........layer\n",
      "............cell\n",
      "...............vars\n",
      "............vars\n",
      ".........vars\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......multiclass_auprc\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "4/4 [==============================] - 1s 34ms/step\n",
      "4/4 [==============================] - 0s 28ms/step\n",
      "4/4 [==============================] - 0s 23ms/step\n",
      "4/4 [==============================] - 0s 22ms/step\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      "4/4 [==============================] - 0s 18ms/step\n",
      "4/4 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 25ms/step\n",
      "4/4 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 19ms/step\n",
      "2/2 [==============================] - 2s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    }
   ],
   "source": [
    "# #%%\n",
    "import glob\n",
    "labpaths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog/*.csv\")\n",
    "realpaths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog/*.csv\")\n",
    "test_paths = labpaths + realpaths\n",
    "from collections import OrderedDict \n",
    "\n",
    "all_preds_df = pd.DataFrame()\n",
    "dict_preds_df = OrderedDict()\n",
    "for f in test_paths:\n",
    "    \n",
    "    df = pd.read_csv(f)\n",
    "    df.set_index('Time', drop=True, inplace=True)\n",
    "    df['Id'] = f.split('/')[-1].split('.')[0]\n",
    "    df['Id'] = df['Id'].astype(str) + '_' + df.index.astype(str)\n",
    "    \n",
    "    for task in ['multiclass']: #binary',\n",
    "        curr_preds_df = pd.DataFrame()\n",
    "        if 'tdcsfog' in f:\n",
    "            #model = config['lab'][task]['model']\n",
    "            #model.load_weights(os.path.join('/kaggle/working','tdcsfog',task,'checkpoint_fold0'))\n",
    "            model = copy.deepcopy(lstm_model)\n",
    "            SAMPLES, jump = 192, 38\n",
    "            unitConversion = 1\n",
    "        elif 'defog' in f:\n",
    "            unitConversion = 9.81\n",
    "            if task == 'binary':\n",
    "                #model = config['realworld'][task]['model']\n",
    "                #model.load_weights(os.path.join('/kaggle/working','defog',task,'checkpoint_fold0'))\n",
    "                model = copy.deepcopy(lstm_model)\n",
    "                SAMPLES, jump = 150, 30\n",
    "            elif task == 'multiclass': #use the tdcsfog (lab) model for this case (might not be ideal)\n",
    "                #model = config['lab'][task]['model']\n",
    "                #model.load_weights(os.path.join('/kaggle/working','tdcsfog',task,'checkpoint_fold0'))\n",
    "                model = copy.deepcopy(lstm_model)\n",
    "                SAMPLES, jump = 192, 38\n",
    "\n",
    "        if task == 'binary':\n",
    "            CATEGORIES = ['Event']\n",
    "        elif task == 'multiclass':\n",
    "            CATEGORIES = ['StartHesitation','Turn','Walking'] # 3 classes\n",
    "    \n",
    "        start = 0\n",
    "        end = start + SAMPLES\n",
    "        chunks_list = []\n",
    "        while end <= df.shape[0]:\n",
    "            chunk_df = df.iloc[start:end,:]\n",
    "            chunk_arr = np.array(chunk_df[FEATNAMES]) # SAMPLES x NFEATS\n",
    "            b,a = signal.butter(2, 15, 'low', fs=128 if 'tdcsfog' in f else 100)\n",
    "            chunk_arr = signal.lfilter(b,a,chunk_arr,axis=0)\n",
    "            chunk_arr = chunk_arr * unitConversion\n",
    "            chunk_arr = np.expand_dims(chunk_arr,0) # 1 x SAMPLES x NFEATS\n",
    "            chunks_list.append(chunk_arr)\n",
    "            start = start + SAMPLES \n",
    "            end = start + SAMPLES\n",
    "        \n",
    "        all_chunks = np.vstack(chunks_list)\n",
    "        batch_size = 128\n",
    "        nbatches = all_chunks.shape[0] // batch_size\n",
    "        remainder = all_chunks.shape[0] % batch_size\n",
    "        if remainder > 0:\n",
    "            nbatches += 1\n",
    "        count = 0 \n",
    "        for i in range(nbatches):\n",
    "            start = i*batch_size\n",
    "            end = start + batch_size\n",
    "            chunk_arr = all_chunks[start:end,:,:] # B x SAMPLES x NFEATS\n",
    "            preds = model.predict(chunk_arr) # B x SAMPLES x NCLASSES\n",
    "            preds_df = pd.DataFrame(preds.reshape(-1,len(CATEGORIES)),columns=CATEGORIES)\n",
    "            curr_preds_df = pd.concat((curr_preds_df,preds_df),0)\n",
    "            count += preds.shape[0]*SAMPLES\n",
    "\n",
    "        # make sure to cover the final (smaller batch)\n",
    "        final_nsamples = df.shape[0] - count\n",
    "        chunk_df = df.iloc[-SAMPLES:,:]\n",
    "        chunk_arr = np.array(chunk_df[FEATNAMES]) # SAMPLES x NFEATS\n",
    "        b,a = signal.butter(2, 15, 'low', fs=128 if 'tdcsfog' in f else 100)\n",
    "        chunk_arr = signal.lfilter(b,a,chunk_arr,axis=0)\n",
    "        chunk_arr = chunk_arr * unitConversion\n",
    "        chunk_arr = np.expand_dims(chunk_arr,0) # 1 x SAMPLES x NFEATS\n",
    "        preds = model.predict(chunk_arr)\n",
    "        preds_df = pd.DataFrame(preds.reshape(-1,len(CATEGORIES)),columns=CATEGORIES)\n",
    "        preds_df = preds_df[-final_nsamples:]\n",
    "        curr_preds_df = pd.concat((curr_preds_df,preds_df),0)\n",
    "        curr_preds_df.index = df.index\n",
    "        curr_preds_df['Id'] = df['Id']\n",
    "        \n",
    "        setting = 'defog' if 'defog' in f else 'tdcsfog'\n",
    "        filename = f.split('/')[-1].split('.')[0]\n",
    "        key = filename + '-' + setting + '-' + task\n",
    "        dict_preds_df[key] = curr_preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6348ee28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:21:56.565188Z",
     "iopub.status.busy": "2023-04-04T02:21:56.564112Z",
     "iopub.status.idle": "2023-04-04T02:21:56.589010Z",
     "shell.execute_reply": "2023-04-04T02:21:56.587109Z"
    },
    "papermill": {
     "duration": 0.064161,
     "end_time": "2023-04-04T02:21:56.592672",
     "exception": false,
     "start_time": "2023-04-04T02:21:56.528511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# # post processing the binary and multiclass predictions\n",
    "all_preds_df = pd.DataFrame()\n",
    "for key in dict_preds_df.keys():\n",
    "    filename,setting,task = key.split('-')\n",
    "    if task == 'multiclass':\n",
    "        #binary_preds_df = dict_preds_df[key.replace('multiclass','binary')]\n",
    "        multi_preds_df = dict_preds_df[key]\n",
    "        #multi_preds_df['event_prediction'] = binary_preds_df['Event']>0 # bool indicating presence of FOG event\n",
    "        #multi_preds_df.loc[multi_preds_df['event_prediction']==False,['StartHesitation','Turn','Walking']] = 0\n",
    "        multi_preds_df = multi_preds_df[['Id','StartHesitation','Turn','Walking']]\n",
    "        all_preds_df = pd.concat((all_preds_df,multi_preds_df),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b76d362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:21:56.663100Z",
     "iopub.status.busy": "2023-04-04T02:21:56.662785Z",
     "iopub.status.idle": "2023-04-04T02:21:56.668939Z",
     "shell.execute_reply": "2023-04-04T02:21:56.667929Z"
    },
    "papermill": {
     "duration": 0.043114,
     "end_time": "2023-04-04T02:21:56.671215",
     "exception": false,
     "start_time": "2023-04-04T02:21:56.628101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# labpaths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog/*.csv\")\n",
    "# realpaths = glob.glob(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/defog/*.csv\")\n",
    "# test_paths = labpaths + realpaths\n",
    "# SAVE_DIR = '/kaggle/working/'\n",
    "# submission = []\n",
    "# for f in test_paths:\n",
    "#     df = pd.read_csv(f)\n",
    "#     df.set_index('Time', drop=True, inplace=True)\n",
    "#     df['Id'] = f.split('/')[-1].split('.')[0]\n",
    "#     df['Id'] = df['Id'].astype(str) + '_' + df.index.astype(str)\n",
    "#     submission.append(df)\n",
    "# submission = pd.concat(submission)\n",
    "# preds_df = pd.DataFrame(submission['Id'])\n",
    "# preds_df[['StartHesitation','Turn','Walking']] = 0\n",
    "#preds_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3704ed0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-04T02:21:56.737509Z",
     "iopub.status.busy": "2023-04-04T02:21:56.737198Z",
     "iopub.status.idle": "2023-04-04T02:21:58.156818Z",
     "shell.execute_reply": "2023-04-04T02:21:58.155682Z"
    },
    "papermill": {
     "duration": 1.45476,
     "end_time": "2023-04-04T02:21:58.159579",
     "exception": false,
     "start_time": "2023-04-04T02:21:56.704819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/'\n",
    "sub = pd.read_csv(p+'sample_submission.csv')\n",
    "sub['t'] = 0\n",
    "submission = pd.merge(sub[['Id','t']], all_preds_df, how='left', on='Id').fillna(0.0)\n",
    "submission[['Id','StartHesitation', 'Turn' , 'Walking']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 143.837684,
   "end_time": "2023-04-04T02:22:01.842582",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-04T02:19:38.004898",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
